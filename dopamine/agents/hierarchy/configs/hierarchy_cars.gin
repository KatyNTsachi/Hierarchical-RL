# Hyperparameters used in Mnih et al. (2015).
import dopamine.discrete_domains.atari_lib
import dopamine.discrete_domains.run_experiment
import dopamine.agents.hierarchy.hierarchy_agent
import dopamine.agents.dqn.dqn_agent
import dopamine.agents.hierarchy.hierarchy_dqn_agent
import dopamine.replay_memory.circular_replay_buffer
import gin.tf.external_configurables

#my param
#how many epocs we want to perform before super agent starts to learn
Runner.epoc_in_pretrain = 1
HierarchyAgent.steps_in_every_action = 50000 #how many steps the sub agent does every time he is chosen
HierarchyAgent.seccond_learning_rate = 0.0

HierarchyAgent.gamma = 0.99
HierarchyAgent.update_horizon = 1

# should be greater then sub agent so they learn first, this is used as #warmup steps in linearly_decaying_epsilon. epsilon is a #function of training steps. subagents do 10 times as much training.
# to get an estimate of number of iterations till epsilonstarts to decay: min_replay_history/training_steps =  80.
# epsilon of subagents starts to decay at: HierarchyDQNAgent.min_replay_history/(10*training_steps) < 1
HierarchyAgent.min_replay_history = 20000 
HierarchyAgent.update_period = 4
HierarchyAgent.target_update_period = 8000  # agent steps
HierarchyAgent.epsilon_train = 0.01
HierarchyAgent.epsilon_eval = 0.001
HierarchyAgent.epsilon_decay_period = 250000    # agent steps
HierarchyAgent.tf_device = '/gpu:0'  # use '/cpu:*' for non-GPU version
HierarchyAgent.optimizer = @tf.train.RMSPropOptimizer()

HierarchyDQNAgent.gamma = 0.99
HierarchyDQNAgent.update_horizon = 1
HierarchyDQNAgent.min_replay_history = 20000  # agent steps
HierarchyDQNAgent.update_period = 4
HierarchyDQNAgent.target_update_period = 8000  # agent steps
HierarchyDQNAgent.epsilon_train = 0.01
HierarchyDQNAgent.epsilon_eval = 0.001
HierarchyDQNAgent.epsilon_decay_period = 250000    # agent steps
HierarchyDQNAgent.tf_device = '/gpu:0'  # use '/cpu:*' for non-GPU version
HierarchyDQNAgent.optimizer = @tf.train.RMSPropOptimizer()

tf.train.RMSPropOptimizer.learning_rate = 0.00025
tf.train.RMSPropOptimizer.decay = 0.95
tf.train.RMSPropOptimizer.momentum = 0.0
tf.train.RMSPropOptimizer.epsilon = 0.00001
tf.train.RMSPropOptimizer.centered = True

atari_lib.create_atari_environment.game_name = 'Breakout'
# Deterministic ALE version used in the DQN Nature paper (Mnih et al., 2015).
atari_lib.create_atari_environment.sticky_actions = True
create_agent.agent_name = 'hierarchy'
Runner.num_iterations = 600
Runner.training_steps = 250000  # agent steps
Runner.evaluation_steps = 125000  # agent steps
Runner.max_steps_per_episode =  27000  # agent steps

#AtariPreprocessing.terminal_on_life_loss = True

WrappedReplayBuffer.replay_capacity = 1000000
WrappedReplayBuffer.batch_size = 32

OutOfGraphReplayBufferSubAgent.replay_capacity = 1000000
OutOfGraphReplayBufferSubAgent.batch_size = 32

